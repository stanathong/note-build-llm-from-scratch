# Chapter 1 - Understanding large language models

## Introduction

* Large language models (LLM) are deep neural network models.
* LLM is a new era for NLP (natural language processing).
* Traditional methods can perform tasks such as email spam classification which can be captured with handcrafted rules or simpler models.
* The tranditional methods underperformed in language tasks that require complex understanding and generating text e.g.
    * parsing detailed instruction
    * conducting contextual analysis
    * creating coherent and contextually appropriate text
* LLMs are capable to **understand, generate, and interpret human language**, in which they can process and generate text in ways that appear coherent and contextually relevant.
* LLMs are trained on vast quantities of text data This large-scale training allows LLMs to capture **deeper contextual information and subtleties of human language**.
* LLMs have improved performance in wide range of NLP tasks such as text translation, sentiment analysis, question answering etc.
* **Another important distinction between contemporary LLMs and earlier NLP models is that earlier NLP models were typically designed for specific tasks such as text categorization, language translation etc. (NLP models exelled in their narrow applications) LLMs demonstrate a broader proficiency across a wide range of NLP tasks**
* The success behind LLMs can be attributed to (1) transformer architecture, (2) the vast amount of data on which LLMs are trained.

## 1.1 What is an LLM?

<img width="741" alt="image" src="https://github.com/user-attachments/assets/58ffed13-5509-42df-b493-2b75745926b6" />

* An LLM is a NN designed to understand, generate and respond to human-like text.
* The term *large* in *large language model* refers to both the model's size in term of parameters and the immense dataset on which it's trained.
* LLM models often have tens or even hundreds of billions of parameters, which are adjustable weights in the network that are optimised during **training to predict the next word in the sequence**.
* LLMs utilise an architecture called the *transformer*, which allows them to pay selective attention to different parts of the input when making predictions.
* Based on the fact that **LLMs can generate text**, LLMs are also referred to as Generative AI or GenAI.



 
