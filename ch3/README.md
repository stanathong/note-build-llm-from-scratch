# Chapter 3: Coding attention mechanisms

## Introduction

## 3.1 The problem with modeling long sequences

## 3.2 Capturing data dependencies with attention mechanisms

## 3.3 Attending to different parts of the input with self-attention

## 3.4 Implementing self-attention with trainable weights

## 3.5 Hiding future words with causal attention

## 3.6 Extending single-head attention to multi-head attention

## Summary
