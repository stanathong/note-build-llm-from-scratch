# Chapter 4: Implementing a GPT model from scratch to generate text

## 4.1 Coding an LLM architecture

## 4.2 Normalizing activations with layer normalization

## 4.3 Implementing a feed forward network with GELU activations

## 4.4 Adding shortcut connections

## 4.5 Connecting attention and linear layers in a transformer block

## 4.6 Coding the GPT model

## 4.7 Generating text

## Summary
